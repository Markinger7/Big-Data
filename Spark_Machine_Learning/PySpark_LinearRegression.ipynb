{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with PySpark\n",
    "\n",
    "<a href='https://spark.apache.org/docs/latest/ml-classification-regression.html#linear-regression'>Link</a> to documentation\n",
    "\n",
    "\n",
    "## Content\n",
    "1. [Example from the documentation](#doc)\n",
    "2. [Example with ecommerce data](#ecom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='doc'></a>\n",
    "## 1. Example from the documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up PySpark\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('lr_example').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the trainind data\n",
    "training = spark.read.format('libsvm').load('sample_linear_regression_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|              label|            features|\n",
      "+-------------------+--------------------+\n",
      "| -9.490009878824548|(10,[0,1,2,3,4,5,...|\n",
      "| 0.2577820163584905|(10,[0,1,2,3,4,5,...|\n",
      "| -4.438869807456516|(10,[0,1,2,3,4,5,...|\n",
      "|-19.782762789614537|(10,[0,1,2,3,4,5,...|\n",
      "| -7.966593841555266|(10,[0,1,2,3,4,5,...|\n",
      "| -7.896274316726144|(10,[0,1,2,3,4,5,...|\n",
      "| -8.464803554195287|(10,[0,1,2,3,4,5,...|\n",
      "| 2.1214592666251364|(10,[0,1,2,3,4,5,...|\n",
      "| 1.0720117616524107|(10,[0,1,2,3,4,5,...|\n",
      "|-13.772441561702871|(10,[0,1,2,3,4,5,...|\n",
      "| -5.082010756207233|(10,[0,1,2,3,4,5,...|\n",
      "|  7.887786536531237|(10,[0,1,2,3,4,5,...|\n",
      "| 14.323146365332388|(10,[0,1,2,3,4,5,...|\n",
      "|-20.057482615789212|(10,[0,1,2,3,4,5,...|\n",
      "|-0.8995693247765151|(10,[0,1,2,3,4,5,...|\n",
      "| -19.16829262296376|(10,[0,1,2,3,4,5,...|\n",
      "|  5.601801561245534|(10,[0,1,2,3,4,5,...|\n",
      "|-3.2256352187273354|(10,[0,1,2,3,4,5,...|\n",
      "| 1.5299675726687754|(10,[0,1,2,3,4,5,...|\n",
      "| -0.250102447941961|(10,[0,1,2,3,4,5,...|\n",
      "+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# in Spark every feature gets vectorized so that there are only two columns for a supervised problem\n",
    "training.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define linear regression\n",
    "lr = LinearRegression()\n",
    "\n",
    "# fit the data\n",
    "lrModel = lr.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([0.0073, 0.8314, -0.8095, 2.4412, 0.5192, 1.1535, -0.2989, -0.5129, -0.6197, 0.6956])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# coefficients\n",
    "lrModel.coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get metrics from model \n",
    "`.summary` can be used to output general metrics of the linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root mean squared error:  10.16309157133015\n",
      "r2:  0.027839179518600154\n",
      "p value:  [0.9927505031240562, 0.30967074330990396, 0.3178269194409711, 0.003972477331573909, 0.5201486327242175, 0.16213017210149872, 0.7102819001865635, 0.5266812209137877, 0.46256007153356316, 0.37825808848978526, 0.7592692146070568]\n"
     ]
    }
   ],
   "source": [
    "# Error of the model\n",
    "rmse = lrModel.summary.rootMeanSquaredError\n",
    "r2 = lrModel.summary.r2\n",
    "pvalue = lrModel.summary.pValues\n",
    "\n",
    "print('root mean squared error: ', rmse)\n",
    "print('r2: ', r2)\n",
    "print('p value: ', pvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data in train and test\n",
    "`.randomSplit` can be used to split the data into train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define split object\n",
    "train, test = training.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|summary|               label|\n",
      "+-------+--------------------+\n",
      "|  count|                 342|\n",
      "|   mean|0.013897725894133819|\n",
      "| stddev|  10.132893652003213|\n",
      "|    min| -28.571478869743427|\n",
      "|    max|  27.111027963108548|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check count of train split\n",
    "train.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|              label|\n",
      "+-------+-------------------+\n",
      "|  count|                159|\n",
      "|   mean| 0.7795489161251338|\n",
      "| stddev|  10.71889983030931|\n",
      "|    min|-20.212077258958672|\n",
      "|    max|  27.78383192005107|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check count of test split \n",
    "test.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_model = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = correct_model.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.956008749492854"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make prediction on unlabeled data\n",
    "call `.transform` on the trained model and give it the unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create unlableled data\n",
    "unlabeled_data = test.select('features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "|(10,[0,1,2,3,4,5,...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unlabeled_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use transform to get the predictions of unlabled data\n",
    "preds = correct_model.transform(unlabeled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            features|          prediction|\n",
      "+--------------------+--------------------+\n",
      "|(10,[0,1,2,3,4,5,...|  2.6583969552816993|\n",
      "|(10,[0,1,2,3,4,5,...|  1.1398125000378085|\n",
      "|(10,[0,1,2,3,4,5,...|  2.1990693041097664|\n",
      "|(10,[0,1,2,3,4,5,...|  0.2795713619866958|\n",
      "|(10,[0,1,2,3,4,5,...| -2.7924028557941023|\n",
      "|(10,[0,1,2,3,4,5,...|  1.2987132064268772|\n",
      "|(10,[0,1,2,3,4,5,...| -1.2429343191114903|\n",
      "|(10,[0,1,2,3,4,5,...| -3.0186387404689357|\n",
      "|(10,[0,1,2,3,4,5,...|  -0.894030222946144|\n",
      "|(10,[0,1,2,3,4,5,...|  1.0074809063787016|\n",
      "|(10,[0,1,2,3,4,5,...|-0.07537250549319433|\n",
      "|(10,[0,1,2,3,4,5,...|-0.17744263622502174|\n",
      "|(10,[0,1,2,3,4,5,...|  1.3685817631905408|\n",
      "|(10,[0,1,2,3,4,5,...|   2.661268475726091|\n",
      "|(10,[0,1,2,3,4,5,...|  0.8671677276779142|\n",
      "|(10,[0,1,2,3,4,5,...|  1.0810631984499863|\n",
      "|(10,[0,1,2,3,4,5,...|  0.0879488502015591|\n",
      "|(10,[0,1,2,3,4,5,...|  -0.839627126562648|\n",
      "|(10,[0,1,2,3,4,5,...|  -2.932291650139565|\n",
      "|(10,[0,1,2,3,4,5,...| -3.2523111378509433|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop session to creat a new one \n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ecom'></a>\n",
    "## 2. Example with ecommerce data\n",
    "Goal: create a model that predicts the yearly spending of a customer for a product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('lr_example_2').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inferSchema = true will automatically infer the column types based on the data\n",
    "# header = when the first row has the column names it should be set to True \n",
    "data = spark.read.csv('Ecommerce_Customers.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Email='mstephenson@fernandez.com', Address='835 Frank TunnelWrightmouth, MI 82180-9605', Avatar='Violet', Avg Session Length=34.49726772511229, Time on App=12.65565114916675, Time on Website=39.57766801952616, Length of Membership=4.0826206329529615, Yearly Amount Spent=587.9510539684005)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Email': 'mstephenson@fernandez.com',\n",
       " 'Address': '835 Frank TunnelWrightmouth, MI 82180-9605',\n",
       " 'Avatar': 'Violet',\n",
       " 'Avg Session Length': 34.49726772511229,\n",
       " 'Time on App': 12.65565114916675,\n",
       " 'Time on Website': 39.57766801952616,\n",
       " 'Length of Membership': 4.0826206329529615,\n",
       " 'Yearly Amount Spent': 587.9510539684005}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the first row in a more readable way\n",
    "data.head().asDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Email: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- Avatar: string (nullable = true)\n",
      " |-- Avg Session Length: double (nullable = true)\n",
      " |-- Time on App: double (nullable = true)\n",
      " |-- Time on Website: double (nullable = true)\n",
      " |-- Length of Membership: double (nullable = true)\n",
      " |-- Yearly Amount Spent: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Yearly Amount Spent` is the label we want to predict with the provided features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform dataframe for machine learning\n",
    "- `Vectors`\n",
    "- `VectorAssembler`: A feature transformer that merges multiple columns into a vector column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Email',\n",
       " 'Address',\n",
       " 'Avatar',\n",
       " 'Avg Session Length',\n",
       " 'Time on App',\n",
       " 'Time on Website',\n",
       " 'Length of Membership',\n",
       " 'Yearly Amount Spent']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine every input feature into a vector which is called 'features'\n",
    "assembler = VectorAssembler(inputCols=['Avg Session Length', 'Time on App', 'Time on Website', 'Length of Membership'], \n",
    "                            outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = assembler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|[34.4972677251122...|\n",
      "|[31.9262720263601...|\n",
      "|[33.0009147556426...|\n",
      "|[34.3055566297555...|\n",
      "|[33.3306725236463...|\n",
      "|[33.8710378793419...|\n",
      "|[32.0215955013870...|\n",
      "|[32.7391429383803...|\n",
      "|[33.9877728956856...|\n",
      "|[31.9365486184489...|\n",
      "|[33.9925727749537...|\n",
      "|[33.8793608248049...|\n",
      "|[29.5324289670579...|\n",
      "|[33.1903340437226...|\n",
      "|[32.3879758531538...|\n",
      "|[30.7377203726281...|\n",
      "|[32.1253868972878...|\n",
      "|[32.3388993230671...|\n",
      "|[32.1878120459321...|\n",
      "|[32.6178560628234...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.select('features').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = output.select('features', 'Yearly Amount Spent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data in train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = final_data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(labelCol='Yearly Amount Spent')\n",
    "\n",
    "lrModel = lr.fit(train)\n",
    "\n",
    "test_results = lrModel.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.736820526313888"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9799957531461406"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results.r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|            features|        prediction|\n",
      "+--------------------+------------------+\n",
      "|[29.5324289670579...|399.56050346986785|\n",
      "|[30.7377203726281...|451.15676799880134|\n",
      "|[30.8364326747734...|471.52828062336107|\n",
      "|[31.3091926408918...| 429.8306938264004|\n",
      "|[31.4252268808548...| 534.9386303187684|\n",
      "|[31.5257524169682...|450.19904893786565|\n",
      "|[31.5316044825729...| 432.0719853344576|\n",
      "|[31.5741380228732...| 559.2845068661395|\n",
      "|[31.5761319713222...| 543.8941230476541|\n",
      "|[31.6005122003032...|461.19030760838723|\n",
      "|[31.6098395733896...| 426.8225711425032|\n",
      "|[31.6610498227460...|417.53312414136303|\n",
      "|[31.6739155032749...|502.59262646203024|\n",
      "|[31.7216523605090...|348.85431836644807|\n",
      "|[31.7366356860502...| 495.8913513131672|\n",
      "|[31.8093003166791...| 549.2532862862183|\n",
      "|[31.8124825597242...| 396.5433068847433|\n",
      "|[31.8164283341993...| 517.9121815165729|\n",
      "|[31.8512531286083...| 464.5393202969494|\n",
      "|[31.8627411090001...| 557.8244286511037|\n",
      "+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create unlableled data\n",
    "unlabeled_data = test.select('features')\n",
    "\n",
    "# use transform to get the predictions of unlabled data\n",
    "preds = lrModel.transform(unlabeled_data)\n",
    "\n",
    "preds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
